{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the employee data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('employee_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column\n",
    "data.rename(columns={'Experience (Years)': 'Experience'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Gender\"]=data[\"Gender\"].astype('string')\n",
    "data[\"Position\"]=data[\"Position\"].astype('string')\n",
    "data.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping ID column\n",
    "# data.drop(columns=['ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test data sets (70% split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data,test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Display the number of rows in each dataset\n",
    "# print(f'Total rows: {len(data)}')\n",
    "# print(f'Training rows: {len(train_data)}')\n",
    "# print(f'Testing rows: {len(test_data)}')\n",
    "\n",
    "# # Save the split datasets into new CSV files\n",
    "# train_data.to_csv('train_employee_data.csv', index=False)\n",
    "# test_data.to_csv('test_employee_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding\n",
    "data_encoded = pd.get_dummies(data,drop_first=True)\n",
    "\n",
    "# Display the encoded DataFrame\n",
    "data_encoded = data_encoded.astype(int)\n",
    "\n",
    "print(data_encoded.shape)\n",
    "print(data_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_encoded.drop(columns=\"Salary\")\n",
    "X_columns=X.columns\n",
    "print(X_columns)\n",
    "Y=data_encoded[\"Salary\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset in training and testing data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of testing and trainig data\n",
    "\n",
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardising features for similar scale\n",
    "\n",
    "scaler=StandardScaler()\n",
    "\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print(X_train)\n",
    "print(\"\\n\")\n",
    "print(f\"shape of trainig data is {X_train.shape}\")\n",
    "print(\"\\n\")\n",
    "print('*'*80)\n",
    "print(\"\\n\")\n",
    "print(X_test)\n",
    "print(\"\\n\")\n",
    "print(f\"shape of testing data is {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array to a DataFrame\n",
    "X_train_df=pd.DataFrame(X_train,columns=X_columns)\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array to a DataFrame\n",
    "X_test_df=pd.DataFrame(X_test,columns=X_columns)\n",
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_train_df=pd.DataFrame(Y_train)\n",
    "Y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_df=pd.DataFrame(Y_train)\n",
    "Y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split datasets into new CSV files\n",
    "X_test_df.to_csv('X_test_df.csv', index=False)\n",
    "X_train_df.to_csv('X_train_df.csv', index=False)\n",
    "Y_test_df.to_csv('Y_test_df.csv', index=False)\n",
    "Y_train_df.to_csv('Y_train_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple linear regression\n",
    "linreg=LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "linreg.fit(X_train,Y_train)\n",
    "\n",
    "#Prediciting bike rental duration\n",
    "y_pred=linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE=mean_absolute_error(Y_test, y_pred)\n",
    "print(f'Mean Absolute Error of model is {round(MAE,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_data=X.sample(5)\n",
    "model=linreg\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions using the model\n",
    "predictions = model.predict(new_data_scaled)\n",
    "\n",
    "\n",
    "#reversing transformations\n",
    "original_data = scaler.inverse_transform(new_data_scaled)\n",
    "original_ID=original_data[:,:1].astype(int)\n",
    "\n",
    "\n",
    "# Convert array to DataFrame with a column name\n",
    "df = pd.DataFrame({\n",
    "    'ID': original_ID.flatten(),  # Flatten to match the (5,) shape\n",
    "    'Predicted_Salary': predictions\n",
    "})\n",
    "\n",
    "\n",
    "# Print DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# categorical = ['Gender', 'Position']\n",
    "# numerical = ['Experience (Years)']\n",
    "# train_dicts = data[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "# dv = DictVectorizer()\n",
    "# X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "# num_columns = X_train.shape[1]\n",
    "# print(\"Number of columns in the feature matrix:\", num_columns)\n",
    "\n",
    "# target = 'Salary'\n",
    "# Y_train = data[target].testues\n",
    "\n",
    "# lr = LinearRegression()\n",
    "# lr.fit(X_train, Y_train)\n",
    "\n",
    "# y_pred = lr.predict(X_train)\n",
    "\n",
    "# mean_squared_error(Y_train, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pred_salaries(inputfile):\n",
    "#     df_sample=pd.read_csv(inputfile)\n",
    "#     sample_dicts = df_sample[categorical + numerical].to_dict(orient='records')\n",
    "#     X_sample = dv.transform(sample_dicts)\n",
    "#     pred_salaries = lr.predict(X_sample)\n",
    "    \n",
    "#     # Create a DataFrame with ID and predicted salaries\n",
    "#     result_df = pd.DataFrame({\n",
    "#         'ID': df_sample[\"ID\"],\n",
    "#         'Predicted_Salary': pred_salaries\n",
    "#     })\n",
    "    \n",
    "#     return result_df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_salaries(\"./employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Create a Pickle file using serialization\n",
    "# import joblib\n",
    "\n",
    "# # Assuming `classifier` is your trained classifier object\n",
    "\n",
    "# # Save the classifier using joblib\n",
    "# joblib.dump(lr, \"regressor.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testidate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_test = pd.read_csv('./test_employee_data.csv')\n",
    "# test_dicts = df_test[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "# X_test = dv.transform(test_dicts)\n",
    "\n",
    "# Y_test = df_test[target].testues\n",
    "\n",
    "# y_pred = lr.predict(X_test)\n",
    "\n",
    "# mean_squared_error(Y_test, y_pred, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the model\n",
    "# with open('model.pkl', 'wb') as f:\n",
    "#     pickle.dump(lr, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3_sm = boto3.client(\"sagemaker\", region_name=\"ap-south-1\")\n",
    "# session = sagemaker.Session()\n",
    "# region = session.boto_session.region_name\n",
    "# bucket = \"mlops-salary-predictor-app\"\n",
    "# print(\"Using Bucket:\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sagemaker takes training data from S3 bucket, so uploading data to S3 bucket\n",
    "# train_path = session.upload_data(path=\"train_employee_data.csv\", bucket=bucket)\n",
    "# test_path = session.upload_data(path=\"test_employee_data.csv\", bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile script.py\n",
    "\n",
    "# import argparse\n",
    "# import os\n",
    "# import json\n",
    "# import sklearn\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score\n",
    "# import joblib\n",
    "# import pathlib\n",
    "# from io import StringIO\n",
    "# import boto3\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def model_fn(model_dir):\n",
    "#     clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "#     return clf\n",
    "\n",
    "# if __name__ =='__main__':\n",
    "\n",
    "#     print(\"[INFO] Extracting arguments\")\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "#     parser.add_argument('--n_estimators', type=int, default=100)\n",
    "#     parser.add_argument('--random_state', type=int, default=0)\n",
    "#     # parser.add_argument('--epochs', type=int, default=10)\n",
    "#     # parser.add_argument('--batch-size', type=int, default=100)\n",
    "#     # parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "\n",
    "#     # an alternative way to load hyperparameters via SM_HPS environment variable.\n",
    "#     # parser.add_argument('--sm-hps', type=json.loads, default=os.environ['SM_HPS'])\n",
    "\n",
    "#     # input data and model directories\n",
    "#     parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "#     parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "#     parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "#     parser.add_argument('--train_file', type=str, default='train_employee_data.csv')\n",
    "#     parser.add_argument('--test_file', type=str, default='test_employee_data.csv')\n",
    "\n",
    "#     args, _ = parser.parse_known_args()\n",
    "\n",
    "#     print(\"SKLearn Version: \", sklearn.__version__)\n",
    "#     print(\"Joblib Version: \", joblib.__version__)\n",
    "\n",
    "#     print(\"[INFO] Reading data\")\n",
    "#     print()\n",
    "#     train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "#     test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "#     features = list(train_df.columns)\n",
    "#     # label = features.pop(-1)\n",
    "    \n",
    "#     print(\"Building training and testing datasets\")\n",
    "#     print()\n",
    "#     X_train = train_df[features]\n",
    "#     X_test = test_df[features]\n",
    "#     Y_train = train_df[label]\n",
    "#     Y_test = test_df[label]\n",
    "\n",
    "#     print('Column order: ')\n",
    "#     print(features)\n",
    "#     print()\n",
    "    \n",
    "#     print(\"Label column is: \",label)\n",
    "#     print()\n",
    "    \n",
    "#     print(\"Data Shape: \")\n",
    "#     print()\n",
    "#     print(\"---- SHAPE OF TRAINING DATA (70%) ----\")\n",
    "#     print(X_train.shape)\n",
    "#     print(Y_train.shape)\n",
    "#     print()\n",
    "#     print(\"---- SHAPE OF TESTING DATA (30%) ----\")\n",
    "#     print(X_test.shape)\n",
    "#     print(Y_test.shape)\n",
    "#     print()\n",
    "    \n",
    "#     print(\"Training RandomForest Model.....\")\n",
    "#     print()\n",
    "#     model = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.random_state, verbose=3, n_jobs=None)\n",
    "#     model.fit(X_train, Y_train)\n",
    "#     print()\n",
    "\n",
    "#     model_path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "#     joblib.dump(model, model_path)\n",
    "#     print(\"Model persisted at \" + model_path)\n",
    "#     print()\n",
    "\n",
    "    \n",
    "#     y_pred_test = model.predict(X_test)\n",
    "#     test_acc = accuracy_score(Y_test,y_pred_test)\n",
    "#     test_rep = classification_report(Y_test,y_pred_test)\n",
    "\n",
    "#     print()\n",
    "#     print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "#     print()\n",
    "#     print(\"Total Rows are: \", X_test.shape[0])\n",
    "#     print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "#     print('[TESTING] Testing Report: ')\n",
    "#     print(test_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
